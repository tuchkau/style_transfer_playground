{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datas\n",
    "import torchvision.transforms as tf\n",
    "import torchvision.utils as tutils\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "\n",
    "from kornia.filters import laplacian\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THE NETWORK ####\n",
    "\n",
    "#Writing the VGG network\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self): #Can have an optional pooling parameter to make it average or max\n",
    "        super(VGG,self).__init__()\n",
    "        ##VGG layers\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        #Pooling Layers : The orignal paper mentioned average Pooling\n",
    "        self.p1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.p2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.p3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.p4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.p5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x, out_params = None):\n",
    "        out = {}\n",
    "        # Building up the VGG net that's going to be used\n",
    "        out['re11'] = F.relu(self.conv1_1(x))\n",
    "        out['re12'] = F.relu(self.conv1_2(out['re11']))\n",
    "        out['p1'] = self.p1(out['re12'])\n",
    "        h_relu1_2 = out['re12']\n",
    "        out['re21'] = F.relu(self.conv2_1(out['p1']))\n",
    "        out['re22'] = F.relu(self.conv2_2(out['re21']))\n",
    "        out['p2'] = self.p2(out['re22'])\n",
    "        h_relu2_2 = out['re22']\n",
    "        out['re31'] = F.relu(self.conv3_1(out['p2']))\n",
    "        out['re32'] = F.relu(self.conv3_2(out['re31']))\n",
    "        out['re33'] = F.relu(self.conv3_3(out['re32']))\n",
    "        out['re34'] = F.relu(self.conv3_4(out['re33']))\n",
    "        out['p3'] = self.p3(out['re34'])\n",
    "        h_relu3_3 = out['re33']\n",
    "        out['re41'] = F.relu(self.conv4_1(out['p3']))\n",
    "        out['re42'] = F.relu(self.conv4_2(out['re41']))\n",
    "        out['re43'] = F.relu(self.conv4_3(out['re42']))\n",
    "        out['re44'] = F.relu(self.conv4_4(out['re43']))\n",
    "        h_relu4_3 = out['re43']\n",
    "        out['p4'] = self.p4(out['re44'])\n",
    "        out['re51'] = F.relu(self.conv5_1(out['p4']))\n",
    "        out['re52'] = F.relu(self.conv5_2(out['re51']))\n",
    "        out['re53'] = F.relu(self.conv5_3(out['re52']))\n",
    "        out['re54'] = F.relu(self.conv5_4(out['re53']))\n",
    "        out['p5'] = self.p5(out['re54'])\n",
    "        if out_params is not None:\n",
    "             return [out[param] for param in out_params]\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preprocessing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer for content loss\n",
    "content_layers = ['re42']\n",
    "#layers for style losses\n",
    "style_layers = ['re11','re21','re31','re41','re51']\n",
    "\n",
    "#setting seed for pytorch\n",
    "#torch.manual_seed(random.randint(1, 10000))\n",
    "#if you want to run the program on cuda then\n",
    "torch.cuda.manual_seed_all(random.randint(1, 10000))\n",
    "\n",
    "if not os.path.exists(\"images/\"):\n",
    "    os.makedirs(\"images/\")\n",
    "    \n",
    "#The below flag allows you to enable the cudnn auto-tuner\n",
    "#to find the best algorithm for your hardware\n",
    "cudnn.benchmark = True\n",
    "\n",
    "#Dataset Processing\n",
    "transform = tf.Compose([\n",
    "    #tf.Resize((512, 512), interpolation=Image.BICUBIC), #Default image_size\n",
    "    tf.ToTensor(), #Transform it to a torch tensor\n",
    "    tf.Lambda(lambda x:x[torch.LongTensor([2, 1, 0])]), #Converting from RGB to BGR\n",
    "    tf.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], std=[0.225, 0.224, 0.229]), #subracting imagenet mean\n",
    "    #tf.Lambda(lambda x: x.mul_(255))\n",
    "])\n",
    "\n",
    "def load_img(path):\n",
    "    img = Image.open(path)\n",
    "    img = Variable(transform(img))\n",
    "    img = img.unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "def save_img(img, path):\n",
    "    post = tf.Compose([\n",
    "         #tf.Lambda(lambda x: x.mul_(1./255)),\n",
    "         tf.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], std=[1,1,1]),\n",
    "         tf.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
    "         ])\n",
    "    img = post(img)\n",
    "    img = img.clamp_(0,1)\n",
    "    tutils.save_image(img,\n",
    "                path,\n",
    "                normalize=True)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        f = input.view(b, c, h*w) #bxcx(hxw)\n",
    "        # torch.bmm(batch1, batch2, out=None)\n",
    "        # batch1 : bxmxp, batch2 : bxpxn -> bxmxn\n",
    "        G = torch.bmm(f, f.transpose(1, 2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n",
    "        return G.div_(h*w)\n",
    "\n",
    "class styleLoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        GramInput = GramMatrix()(input)\n",
    "        return nn.MSELoss()(GramInput, target)\n",
    "\n",
    "class STLoss(nn.Module):\n",
    "    def forward(self, input):\n",
    "        up = input[:,:,:-1,:]\n",
    "        down = input[:,:,1:,:]\n",
    "        left = input[:,:,:,:-1]\n",
    "        right = input[:,:,:,1:]\n",
    "        l = nn.L1Loss()\n",
    "        return l(up,down) + l(left,right)\n",
    "  \n",
    "class EdgeLoss(nn.Module):\n",
    "    def __init__(self, kernel=3):\n",
    "        super(EdgeLoss, self).__init__()\n",
    "        self.kernel=kernel\n",
    "    def forward(self, input, target):\n",
    "        pool = torch.nn.MaxPool2d(2)\n",
    "        target_e = laplacian(pool(target), self.kernel)\n",
    "        input_e = laplacian(pool(input), self.kernel)\n",
    "        return nn.MSELoss()(target_e, input_e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating desired set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'content': [4], #numbers of content image, content path should looks like content/content_{num}.jpg\n",
    "    'style': [2, 4], #numbers of style image, style path should looks like style/style_{num}.jpg\n",
    "    'sw': [1], #weights for style loss\n",
    "    'cw': [2], #weights for content loss\n",
    "    'gamma': [1], #weights for edge detection loss\n",
    "    'thetha': [3], #weights for total variation loss\n",
    "    'kernel': [7] #Must be odd (kernel sizes for Laplacian filter)\n",
    "}\n",
    "keys, values = zip(*parameters.items())\n",
    "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (p1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (p2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (p3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (p4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (p5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_directory = \"vgg_conv.pth\" #path to pretrained vgg vgg_directory\n",
    "vgg = VGG()\n",
    "vgg.load_state_dict(torch.load(vgg_directory))\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg.cuda() # Putting model on cuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb44bee997d4082940d3ec4208ef357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445bebb7755a4905894030deac062ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=99.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc4770987c4418fbb9cadfe851333ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=99.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for perm in tqdm(permutations_dicts):\n",
    "    style_num = perm['style']\n",
    "    content_num = perm['content']\n",
    "\n",
    "    style_img = f\"./style/style_{style_num}.jpg\"\n",
    "    content_img = f\"./content/content_{content_num}.jpg\"\n",
    "\n",
    "    style_weight = perm['sw']\n",
    "    content_weight = perm['cw']\n",
    "\n",
    "    gamma = perm['gamma']\n",
    "    kernel = perm['kernel']\n",
    "    thetha = perm['thetha']\n",
    "\n",
    "    styleImg = load_img(style_img)\n",
    "    contentImg = load_img(content_img)\n",
    "\n",
    "    #for running on cuda\n",
    "    styleImg = styleImg.cuda()\n",
    "    contentImg = contentImg.cuda()\n",
    "    \n",
    "    styleTargets = []\n",
    "    for t in vgg(styleImg, style_layers):\n",
    "        t = t.detach()\n",
    "        styleTargets.append(GramMatrix()(t))\n",
    "\n",
    "    contentTargets = []\n",
    "    for t in vgg(contentImg, content_layers):\n",
    "        t = t.detach()\n",
    "        contentTargets.append(t)\n",
    "\n",
    "    style_Losses = [styleLoss()] * len(style_layers)\n",
    "\n",
    "    content_Losses = [nn.MSELoss()] * len(content_layers)\n",
    "    st_loss = STLoss()\n",
    "    e_loss = EdgeLoss(kernel)\n",
    "    # We only need to go through the vgg once to get all style and content losses\n",
    "\n",
    "    losses = style_Losses + content_Losses\n",
    "    targets = styleTargets + contentTargets\n",
    "    loss_layers = style_layers + content_layers\n",
    "\n",
    "    weights = [style_weight] * len(style_layers) + [content_weight] * len(content_layers)\n",
    "\n",
    "    optimImg = Variable(torch.rand_like(contentImg.data.clone()), requires_grad=True)\n",
    "    optimizer = optim.LBFGS([optimImg])\n",
    "\n",
    "    #Shifting everything to cuda\n",
    "    for loss in losses:\n",
    "        loss = loss.cuda()\n",
    "    optimImg.cuda()\n",
    "\n",
    "    # Training\n",
    "    no_iter = 100\n",
    "\n",
    "    for iteration in tqdm(range(1, no_iter)):\n",
    "        #print('Iteration [%d]/[%d]'%(iteration,no_iter))\n",
    "        def cl():\n",
    "            optimizer.zero_grad()\n",
    "            out = vgg(optimImg, loss_layers)\n",
    "            totalLossList = []\n",
    "            for i in range(len(out)):\n",
    "                layer_output = out[i]\n",
    "                loss_i = losses[i]\n",
    "                target_i = targets[i]\n",
    "                totalLossList.append(loss_i(layer_output, target_i) * weights[i])\n",
    "            totalLossList.append(thetha*st_loss(optimImg))\n",
    "            totalLossList.append(gamma * e_loss(optimImg, contentImg))\n",
    "            totalLoss = sum(totalLossList)\n",
    "            totalLoss.backward()\n",
    "            #print('Loss: %f'%(totalLoss.data.item())) #Uncomment me if you want to take a look on loss values\n",
    "            return totalLoss\n",
    "        optimizer.step(cl)\n",
    "    outImg = optimImg.data[0].cpu()\n",
    "    path = f'images/cn_{content_num}_st_{style_num}_sw_{style_weight}_cw_{content_weight}_gam_{gamma}_th_{thetha}_ker_{kernel}.jpg'\n",
    "    save_img(outImg.squeeze(), path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style_env",
   "language": "python",
   "name": "style_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
